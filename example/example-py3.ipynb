{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import model.labeled_lda as llda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data\n",
    "labeled_documents = [(\"example example example example example\"*10, [\"example\"]),\n",
    "                     (\"test llda model test llda model test llda model\"*10, [\"test\", \"llda_model\"]),\n",
    "                     (\"example test example test example test example test\"*10, [\"example\", \"test\"]),\n",
    "                     (\"good perfect good good perfect good good perfect good \"*10, [\"positive\"]),\n",
    "                     (\"bad bad down down bad bad down\"*10, [\"negative\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeled-LDA Model:\n",
      "\tK = 6\n",
      "\tM = 5\n",
      "\tT = 12\n",
      "\tWN = 344\n",
      "\tLN = 7\n",
      "\talpha = 0.01\n",
      "\teta = 0.001\n",
      "\tperplexity = 3.9609883743863814\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "print(llda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration: 0, perplexity: 3.9609883743863814\n",
      "gibbs sample count:  344\n",
      "after iteration: 1, perplexity: 2.983007701812946\n",
      "gibbs sample count:  344\n",
      "after iteration: 2, perplexity: 2.663573579993501\n",
      "gibbs sample count:  344\n",
      "after iteration: 3, perplexity: 2.568178112230102\n",
      "gibbs sample count:  344\n",
      "after iteration: 4, perplexity: 2.565333093051932\n",
      "gibbs sample count:  344\n",
      "after iteration: 5, perplexity: 2.560923432893521\n",
      "gibbs sample count:  344\n",
      "after iteration: 6, perplexity: 2.568083596309752\n",
      "gibbs sample count:  344\n",
      "after iteration: 7, perplexity: 2.5498941703336913\n",
      "gibbs sample count:  344\n",
      "after iteration: 8, perplexity: 2.546619432300894\n",
      "gibbs sample count:  344\n",
      "after iteration: 9, perplexity: 2.5546887674986776\n",
      "gibbs sample count:  344\n"
     ]
    }
   ],
   "source": [
    "llda_model.training(iteration=10, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_objet:: \n",
      "alpha_vector [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "eta_vector [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "terms ['llda', 'bad', 'good', 'modeltest', 'perfect', 'exampleexample', 'down', 'model', 'testexample', 'example', 'downbad', 'test']\n",
      "vocabulary {'llda': 0, 'bad': 1, 'good': 2, 'modeltest': 3, 'perfect': 4, 'exampleexample': 5, 'down': 6, 'model': 7, 'testexample': 8, 'example': 9, 'downbad': 10, 'test': 11}\n",
      "topics ['common_topic', 'llda_model', 'negative', 'example', 'test', 'positive']\n",
      "topic_vocabulary {'common_topic': 0, 'llda_model': 1, 'negative': 2, 'example': 3, 'test': 4, 'positive': 5}\n",
      "W [[9, 9, 9, 9, 5, 9, 9, 9, 5, 9, 9, 9, 5, 9, 9, 9, 5, 9, 9, 9, 5, 9, 9, 9, 5, 9, 9, 9, 5, 9, 9, 9, 5, 9, 9, 9, 5, 9, 9, 9, 9], [11, 0, 7, 11, 0, 7, 11, 0, 3, 0, 7, 11, 0, 7, 11, 0, 3, 0, 7, 11, 0, 7, 11, 0, 3, 0, 7, 11, 0, 7, 11, 0, 3, 0, 7, 11, 0, 7, 11, 0, 3, 0, 7, 11, 0, 7, 11, 0, 3, 0, 7, 11, 0, 7, 11, 0, 3, 0, 7, 11, 0, 7, 11, 0, 3, 0, 7, 11, 0, 7, 11, 0, 3, 0, 7, 11, 0, 7, 11, 0, 7], [9, 11, 9, 11, 9, 11, 9, 8, 11, 9, 11, 9, 11, 9, 8, 11, 9, 11, 9, 11, 9, 8, 11, 9, 11, 9, 11, 9, 8, 11, 9, 11, 9, 11, 9, 8, 11, 9, 11, 9, 11, 9, 8, 11, 9, 11, 9, 11, 9, 8, 11, 9, 11, 9, 11, 9, 8, 11, 9, 11, 9, 11, 9, 8, 11, 9, 11, 9, 11, 9, 11], [2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2, 2, 4, 2], [1, 1, 6, 6, 1, 1, 10, 1, 6, 6, 1, 1, 10, 1, 6, 6, 1, 1, 10, 1, 6, 6, 1, 1, 10, 1, 6, 6, 1, 1, 10, 1, 6, 6, 1, 1, 10, 1, 6, 6, 1, 1, 10, 1, 6, 6, 1, 1, 10, 1, 6, 6, 1, 1, 10, 1, 6, 6, 1, 1, 6]]\n",
      "Z [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1], [3, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]\n",
      "K 6\n",
      "M 5\n",
      "T 12\n",
      "WN 344\n",
      "LN 7\n",
      "iteration 10\n",
      "Lambda None\n"
     ]
    }
   ],
   "source": [
    "# save to disk\n",
    "save_model_dir = \"../data/model\"\n",
    "# llda_model.save_model_to_dir(save_model_dir, save_derivative_properties=True)\n",
    "llda_model.save_model_to_dir(save_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 terms of topic 'negative':  ['bad', 'down', 'downbad', 'llda', 'good']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top-5 terms of topic 'negative': \", llda_model.top_terms_of_topic(\"negative\", 5, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
